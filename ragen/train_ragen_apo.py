import torch
import torch.optim as optim
import numpy as np
import yaml
import os
from collections import deque
import time
import warnings
warnings.filterwarnings('ignore')

from .qwen_agent import QwenRAGENAgent
from .experience_buffer import ExperienceBuffer  
from .webshop_env import WebShopEnv
from .reward_calculator import RewardCalculator

# ç®€åŒ–APOè®­ç»ƒå™¨ï¼ˆé¿å…å¤æ‚ä¾èµ–ï¼‰
class SimpleAPOTrainer:
    def __init__(self, beta=0.1, gamma=0.99, cache_file="vstar_cache.pkl", num_vstar_samples=100):
        self.beta = beta
        self.gamma = gamma
        
    def compute_advantages(self, observations, rewards, dones, reference_agent, agent):
        """ç®€åŒ–ä¼˜åŠ¿è®¡ç®—"""
        advantages = []
        v_star_values = []
        
        for i in range(len(rewards)):
            # ç®€åŒ–ä¼˜åŠ¿è®¡ç®—ï¼šä½¿ç”¨å¥–åŠ±ä½œä¸ºåŸºç¡€
            advantage = rewards[i] * 2.0  # æ”¾å¤§å¥–åŠ±ä¿¡å·
            advantages.append(advantage)
            v_star_values.append(rewards[i] * 1.5)  # ç®€åŒ–V*å€¼
            
        return torch.FloatTensor(advantages), torch.FloatTensor(v_star_values)
    
    def compute_policy_loss(self, log_probs, advantages, ref_log_probs):
        """ç®€åŒ–ç­–ç•¥æŸå¤±è®¡ç®—"""
        if isinstance(log_probs, list):
            log_probs = torch.FloatTensor(log_probs)
        if isinstance(ref_log_probs, list):
            ref_log_probs = torch.FloatTensor(ref_log_probs)
            
        # ç­–ç•¥æ¢¯åº¦æŸå¤±
        pg_loss = -(log_probs * advantages).mean()
        
        # KLæ•£åº¦æƒ©ç½šï¼ˆç®€åŒ–ï¼‰
        kl_penalty = torch.nn.functional.kl_div(
            torch.softmax(log_probs, dim=0),
            torch.softmax(ref_log_probs, dim=0),
            reduction='batchmean'
        )
        
        # æ€»æŸå¤±
        total_loss = pg_loss + self.beta * kl_penalty
        
        return total_loss, pg_loss.item(), kl_penalty.item()

class RAGENWebShopTrainer:
    def __init__(self, config_path="configs/webshop_config.yaml"):
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        print("=" * 60)
        print("RAGEN + A*PO + Qwen WebShop è®­ç»ƒç³»ç»Ÿ")
        print("=" * 60)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.env = WebShopEnv(
            server_url=self.config['environment']['server_url'],
            max_steps=self.config['environment']['max_steps']
        )
        
        self.agent = QwenRAGENAgent(
            model_name=self.config['model']['base_model'],
            device=self.config['model']['device']
        )
        
        # å‚è€ƒç­–ç•¥ï¼ˆå›ºå®šï¼‰
        self.reference_agent = QwenRAGENAgent(
            model_name=self.config['model']['base_model'],
            device=self.config['model']['device']
        )
        
        self.reward_calculator = RewardCalculator()
        self.optimizer = optim.Adam(self.agent.parameters(), lr=self.config['training']['learning_rate'])
        self.buffer = ExperienceBuffer(self.config['buffer']['capacity'])
        self.apo_trainer = SimpleAPOTrainer(
            beta=self.config['training']['beta'],
            gamma=self.config['training']['gamma'],
            cache_file=self.config['vstar_cache']['cache_file'],
            num_vstar_samples=self.config['vstar_cache']['num_vstar_samples']
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = deque(maxlen=20)
        self.success_rates = deque(maxlen=20)
        self.format_success_rates = deque(maxlen=20)  # æ ¼å¼æˆåŠŸç‡
        self.best_success_rate = 0.0
        self.total_steps = 0
        
    def collect_experience(self, num_episodes=2):
        """æ”¶é›†ç»éªŒæ•°æ®"""
        print(f"\nğŸ“¥ æ”¶é›† {num_episodes} ä¸ªå›åˆçš„ç»éªŒ...")
        
        for episode in range(num_episodes):
            try:
                obs, info = self.env.reset()
                instruction = info['instruction']
                episode_reward = 0
                done = False
                steps = 0
                
                print(f"\n--- å›åˆ {episode+1} ---")
                print(f"ä»»åŠ¡: {instruction}")
                
                while not done and steps < self.config['environment']['max_steps']:
                    # Qwenç”Ÿæˆæ€è€ƒå’ŒåŠ¨ä½œ
                    think_content, action_content, log_prob, full_response = self.agent.generate_webshop_response(obs, instruction)
                    
                    print(f"\næ­¥éª¤ {steps+1}:")
                    print(f"æ€è€ƒ: {think_content}")
                    print(f"åŠ¨ä½œ: {action_content}")
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    next_obs, env_reward, done, info = self.env.step(action_content, info['session_id'])
                    
                    # è®¡ç®—è¯¦ç»†å¥–åŠ± - ä¿®å¤å‚æ•°é”™è¯¯
                    task_success = (env_reward > 0.5)
                    try:
                        reward = self.reward_calculator.calculate_reward(
                            think_content, 
                            action_content, 
                            next_obs, 
                            task_success,
                            instruction,  # æ·»åŠ ä»»åŠ¡æŒ‡ä»¤
                            steps + 1     # æ·»åŠ æ­¥éª¤æ•°
                        )
                    except TypeError as e:
                        print(f"âš ï¸ ä½¿ç”¨ç®€åŒ–å¥–åŠ±è®¡ç®—: {e}")
                        # å¦‚æœå‚æ•°ä¸åŒ¹é…ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                        reward = self.reward_calculator.calculate_simple_reward(
                            think_content, 
                            action_content, 
                            task_success
                        )
                    
                    episode_reward += reward
                    steps += 1
                    self.total_steps += 1
                    
                    # å­˜å‚¨ç»éªŒ
                    self.buffer.push(obs, instruction, think_content, action_content, reward, done, log_prob)
                    
                    obs = next_obs
                    
                    if done:
                        break
                
                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                self.episode_rewards.append(episode_reward)
                success = 1 if episode_reward > 0.8 else 0  # æé«˜æˆåŠŸé˜ˆå€¼
                self.success_rates.append(success)
                
                # æ ¼å¼æˆåŠŸç‡ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰- ä½¿ç”¨æ”¹è¿›çš„æ£€æŸ¥æ–¹æ³•
                format_success = 1 if self._check_format_success(think_content, action_content) else 0
                self.format_success_rates.append(format_success)
                
                current_success = np.mean(self.success_rates) if self.success_rates else 0
                current_format_success = np.mean(self.format_success_rates) if self.format_success_rates else 0
                
                print(f"\nå›åˆç»“æœ: æ€»å¥–åŠ±={episode_reward:.2f}, æˆåŠŸç‡={current_success:.3f}, æ ¼å¼æˆåŠŸç‡={current_format_success:.3f}")
                
            except Exception as e:
                print(f"âŒ å›åˆ {episode+1} å‡ºé”™: {e}")
                continue
    
    def _check_format_success(self, think_content, action_content):
        """æ”¹è¿›çš„æ ¼å¼æ£€æŸ¥ - æ›´å®½æ¾ä½†æœ‰æ•ˆ"""
        # æ£€æŸ¥æ€è€ƒå†…å®¹æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯æ¨¡æ¿æ–‡å­—ï¼‰
        valid_think = (think_content and 
                       len(think_content.strip()) > 5 and
                       "ä½ çš„æ¨ç†" not in think_content and
                       "è¯·æ€è€ƒ" not in think_content and
                       "æ€è€ƒè¿‡ç¨‹" not in think_content and
                       "æ€è€ƒ:" not in think_content)
        
        # æ£€æŸ¥åŠ¨ä½œå†…å®¹æ˜¯å¦æœ‰æ•ˆä¸”å…·ä½“
        valid_action = (action_content and 
                        any(x in action_content for x in ['search[', 'click[', 'buy[']) and
                        action_content != "search[product]" and
                        len(action_content) > 8)  # ç¡®ä¿ä¸æ˜¯å¤ªçŸ­
        
        return valid_think and valid_action
    
    def train_step(self):
        """æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤"""
        if len(self.buffer) < self.config['training']['batch_size']:
            print(f"âš ï¸ ç¼“å†²åŒºä¸è¶³: {len(self.buffer)}/{self.config['training']['batch_size']}")
            return None
            
        batch = self.buffer.sample(self.config['training']['batch_size'])
        if batch is None:
            print("âŒ æ‰¹æ¬¡é‡‡æ ·å¤±è´¥")
            return None
        
        try:
            # è®¡ç®—A*POä¼˜åŠ¿
            advantages, v_star_values = self.apo_trainer.compute_advantages(
                batch['observations'], batch['rewards'], batch['dones'],
                self.reference_agent, self.agent
            )
            
            # è®¡ç®—å‚è€ƒç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
            with torch.no_grad():
                ref_log_probs = []
                for (obs, instruction) in batch['observations']:
                    _, _, ref_log_prob, _ = self.reference_agent.generate_webshop_response(obs, instruction)
                    ref_log_probs.append(ref_log_prob)
                ref_log_probs = torch.FloatTensor(ref_log_probs)
            
            # å½“å‰ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
            current_log_probs = torch.FloatTensor(batch['log_probs'])
            
            # è®¡ç®—A*POç­–ç•¥æŸå¤±
            policy_loss, pg_loss, kl_penalty = self.apo_trainer.compute_policy_loss(
                current_log_probs, advantages, ref_log_probs
            )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            policy_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.agent.parameters(), self.config['training']['grad_clip'])
            self.optimizer.step()
            
            return {
                'total_loss': policy_loss.item(),
                'policy_loss': pg_loss,
                'kl_penalty': kl_penalty,
                'avg_advantage': advantages.mean().item(),
                'avg_reward': np.mean(batch['rewards'])
            }
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒæ­¥éª¤å‡ºé”™: {e}")
            return None
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
        print("æˆåŠŸæ ‡å‡†: æˆåŠŸç‡ä»0%æå‡åˆ°20%+")
        print("é‡ç‚¹è§‚å¯Ÿ: Base Modelå­¦ä¹ æ ¼å¼éµå¾ªèƒ½åŠ›")
        print("-" * 50)
        
        start_time = time.time()
        
        for epoch in range(self.config['training']['num_epochs']):
            print(f"\nğŸ”„ Epoch {epoch + 1}/{self.config['training']['num_epochs']}")
            
            # é˜¶æ®µ1: æ”¶é›†ç»éªŒ
            self.collect_experience(num_episodes=2)
            
            # é˜¶æ®µ2: è®­ç»ƒ
            if len(self.buffer) >= self.config['training']['batch_size']:
                loss_info = self.train_step()
                
                if loss_info:
                    current_success = np.mean(self.success_rates) if self.success_rates else 0
                    current_format = np.mean(self.format_success_rates) if self.format_success_rates else 0
                    
                    print(f"Epoch {epoch:3d} | Loss: {loss_info['total_loss']:7.4f} | "
                          f"Reward: {loss_info['avg_reward']:5.3f} | "
                          f"Success: {current_success:5.3f} | Format: {current_format:5.3f} | "
                          f"Buffer: {len(self.buffer):2d}")
                else:
                    print(f"Epoch {epoch:3d} | è®­ç»ƒè·³è¿‡ - æ— æœ‰æ•ˆæ‰¹æ¬¡")
            
            # é˜¶æ®µ3: è¯„ä¼°å’Œæ£€æŸ¥åœæ­¢æ¡ä»¶
            if epoch % 5 == 0:  # æ›´é¢‘ç¹çš„è¯„ä¼°
                current_success = np.mean(self.success_rates) if self.success_rates else 0
                current_format = np.mean(self.format_success_rates) if self.format_success_rates else 0
                training_time = (time.time() - start_time) / 60
                
                if current_success > self.best_success_rate:
                    self.best_success_rate = current_success
                    print(f"ğŸ¯ æ–°çš„æœ€ä½³æˆåŠŸç‡: {self.best_success_rate:.3f}")
                
                print(f"\n=== è¯„ä¼° Epoch {epoch} ===")
                print(f"è®­ç»ƒæ—¶é—´: {training_time:6.1f} åˆ†é’Ÿ")
                print(f"æ€»æ­¥æ•°: {self.total_steps:6d}")
                print(f"å½“å‰æˆåŠŸç‡: {current_success:6.3f}")
                print(f"æ ¼å¼æˆåŠŸç‡: {current_format:6.3f}")
                print(f"å†å²æœ€ä½³: {self.best_success_rate:6.3f}")
                
                # æˆåŠŸæ ‡å‡†æ£€æŸ¥
                if current_success >= 0.20:
                    print("ğŸ‰" * 20)
                    print("è¾¾åˆ°Part 2ä½œä¸šè¦æ±‚: æˆåŠŸç‡ > 20%!")
                    print("Base ModelæˆåŠŸå­¦ä¹ äº†æ ¼å¼éµå¾ªå’Œä»»åŠ¡è§£å†³!")
                    print("å¯ä»¥åœæ­¢è®­ç»ƒå¹¶å‡†å¤‡æ¼”ç¤º")
                    print("ğŸ‰" * 20)
                    break
                    
                print("-" * 40)
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = (time.time() - start_time) / 60
        final_success = np.mean(self.success_rates) if self.success_rates else 0
        final_format = np.mean(self.format_success_rates) if self.format_success_rates else 0
        
        print(f"\n" + "=" * 50)
        print("è®­ç»ƒå®Œæˆ!")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f} åˆ†é’Ÿ")
        print(f"æœ€ç»ˆæˆåŠŸç‡: {final_success:.3f}")
        print(f"æœ€ç»ˆæ ¼å¼æˆåŠŸç‡: {final_format:.3f}")
        print(f"å†å²æœ€ä½³æˆåŠŸç‡: {self.best_success_rate:.3f}")
        print(f"æ€»è®­ç»ƒæ­¥æ•°: {self.total_steps}")
        print("=" * 50)
        
        self.env.close()

def main():
    # åˆ›å»ºç›®å½•
    os.makedirs("configs", exist_ok=True)
    os.makedirs("ragen", exist_ok=True)
    
    # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    config_path = "configs/webshop_config.yaml"
    if not os.path.exists(config_path):
        default_config = {
            'model': {
                'base_model': "Qwen/Qwen2.5-1.5B",
                'device': "cuda"
            },
            'environment': {
                'server_url': "http://localhost:3000",
                'max_steps': 15
            },
            'training': {
                'learning_rate': 1e-5,
                'batch_size': 4,
                'num_epochs': 50,
                'beta': 0.1,
                'gamma': 0.99,
                'grad_clip': 1.0
            },
            'buffer': {
                'capacity': 1000
            },
            'vstar_cache': {
                'cache_file': "vstar_cache.pkl",
                'num_vstar_samples': 100
            }
        }
        
        with open(config_path, 'w') as f:
            yaml.dump(default_config, f)
        print(f"ğŸ“ åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {config_path}")
    
    trainer = RAGENWebShopTrainer(config_path)
    trainer.train()

if __name__ == "__main__":
    main()