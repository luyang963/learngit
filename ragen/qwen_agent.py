import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
import re
import numpy as np

class QwenRAGENAgent(nn.Module):
    def __init__(self, model_name="Qwen/Qwen2.5-1.5B", device="cuda"):
        super().__init__()
        self.device = device
        
        print(f"åŠ è½½Qwen Baseæ¨¡å‹: {model_name}")
        # åŠ è½½Base Modelç”¨äºæ–‡æœ¬ç”Ÿæˆ
        self.llm = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print("Qwenæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def generate_webshop_response(self, observation, instruction):
        """ç”ŸæˆWebShopä»»åŠ¡çš„æ€è€ƒå’ŒåŠ¨ä½œ - æ”¹è¿›ç‰ˆæœ¬"""
        # æ›´æ¸…æ™°ã€æ›´å…·ä½“çš„Promptï¼Œé¿å…æ¨¡æ¿æ–‡å­—
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªç½‘é¡µè´­ç‰©åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç½‘é¡µå†…å®¹å’Œä»»åŠ¡è¦æ±‚å®Œæˆä»»åŠ¡ã€‚

ç½‘é¡µå†…å®¹: {observation}
ä»»åŠ¡: {instruction}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼æ€è€ƒå’Œè¡ŒåŠ¨ï¼š

<think>
é¦–å…ˆåˆ†æå½“å‰ç½‘é¡µæœ‰ä»€ä¹ˆå†…å®¹ï¼Œç„¶åæ ¹æ®ä»»åŠ¡è¦æ±‚å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
ä¾‹å¦‚ï¼šç½‘é¡µæ˜¾ç¤ºæœç´¢é¡µé¢ï¼Œæˆ‘éœ€è¦æœç´¢"è“è‰²ç‰›ä»”è£¤"ã€‚
</think>
<action>
search[å…·ä½“å•†å“å…³é”®è¯] æˆ– click[å•†å“ID] æˆ– buy[å•†å“ID]
</action>

é‡è¦ï¼šä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ï¼Œä¸¥æ ¼æŒ‰ç…§ä¸Šé¢çš„æ ¼å¼ã€‚

ç°åœ¨å¼€å§‹ï¼š
<think>
"""
        
        inputs = self.tokenizer(prompt, return_tensors="pt", return_attention_mask=True, max_length=512, truncation=True).to(self.device)
        
        with torch.no_grad():
            outputs = self.llm.generate(
                **inputs,
                max_new_tokens=200,  # å¢åŠ tokenæ•°é‡ç¡®ä¿å®Œæ•´è¾“å‡º
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                return_dict_in_generate=True,
                output_scores=True,
                repetition_penalty=1.1,  # å‡å°‘é‡å¤
                no_repeat_ngram_size=3   # é¿å…é‡å¤çŸ­è¯­
            )
        
        full_response = self.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)
        
        print(f"ğŸ” å®Œæ•´å“åº”: {full_response}")
        
        # æ”¹è¿›çš„å†…å®¹æå–
        think_content = self._extract_think_content(full_response)
        action_content = self._extract_action_content(full_response, instruction)
        
        # è®¡ç®—å¯¹æ•°æ¦‚ç‡
        log_prob = self._calculate_log_prob(outputs, inputs.input_ids.size(1))
        
        return think_content, action_content, log_prob, full_response
    
    def _extract_think_content(self, text):
        """æ”¹è¿›çš„æ€è€ƒå†…å®¹æå–"""
        if not text:
            return "åˆ†æä»»åŠ¡éœ€æ±‚å¹¶é‡‡å–è¡ŒåŠ¨"
        
        # æ–¹æ³•1: æå– <think> æ ‡ç­¾å†…å®¹
        think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL)
        if think_match:
            content = think_match.group(1).strip()
            # è¿‡æ»¤æ‰æ¨¡æ¿æ–‡å­—å’Œæ— æ•ˆå†…å®¹
            if (content and 
                len(content) > 5 and 
                "ä½ çš„æ¨ç†" not in content and 
                "è¯·æ€è€ƒ" not in content and
                "æ€è€ƒè¿‡ç¨‹" not in content):
                return content
        
        # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œå°è¯•æ‰¾åˆ°åˆç†çš„æ€è€ƒå†…å®¹
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if (line and 
                len(line) > 10 and 
                not line.startswith('<') and 
                not line.endswith('>') and
                "æ€è€ƒ" not in line and
                "ä½ çš„æ¨ç†" not in line and
                "è¯·æ€è€ƒ" not in line and
                "åŠ¨ä½œ" not in line and
                "search[" not in line and
                "click[" not in line and
                "buy[" not in line):
                return line
        
        # æ–¹æ³•3: è¿”å›æœ‰æ„ä¹‰çš„é»˜è®¤æ€è€ƒ
        return "æ ¹æ®ä»»åŠ¡éœ€æ±‚ï¼Œæˆ‘éœ€è¦æœç´¢ç›¸å…³å•†å“"
    
    def _extract_action_content(self, text, instruction):
        """æ”¹è¿›çš„åŠ¨ä½œå†…å®¹æå–"""
        if not text:
            return self._generate_default_action(instruction)
        
        # æ–¹æ³•1: æå– <action> æ ‡ç­¾å†…å®¹
        action_match = re.search(r'<action>(.*?)</action>', text, re.DOTALL)
        if action_match:
            action = action_match.group(1).strip()
            if self._is_valid_action(action):
                return action
        
        # æ–¹æ³•2: åœ¨æ–‡æœ¬ä¸­æœç´¢åŠ¨ä½œæ¨¡å¼
        action_patterns = [
            r'search\[[^\]]+\]',
            r'click\[\d+\]', 
            r'buy\[\d+\]'
        ]
        
        for pattern in action_patterns:
            match = re.search(pattern, text)
            if match:
                action = match.group(0)
                if self._is_valid_action(action):
                    return action
        
        # æ–¹æ³•3: ç”ŸæˆåŸºäºä»»åŠ¡çš„å…·ä½“åŠ¨ä½œ
        return self._generate_default_action(instruction)
    
    def _is_valid_action(self, action):
        """æ£€æŸ¥åŠ¨ä½œæ˜¯å¦æœ‰æ•ˆ"""
        if not action:
            return False
        
        # æ£€æŸ¥åŠ¨ä½œæ ¼å¼
        valid_formats = [
            r"^search\[.+\]$",
            r"^click\[\d+\]$", 
            r"^buy\[\d+\]$"
        ]
        
        for pattern in valid_formats:
            if re.match(pattern, action.strip()):
                return True
        
        return False
    
    def _generate_default_action(self, instruction):
        """æ ¹æ®ä»»åŠ¡ç”Ÿæˆå…·ä½“çš„é»˜è®¤åŠ¨ä½œ"""
        instruction_lower = instruction.lower()
        
        if "blanket" in instruction_lower and "classic" in instruction_lower:
            return "search[classic wool blanket]"
        elif "jeans" in instruction_lower and "blue" in instruction_lower:
            if "32" in instruction_lower:
                return "search[blue jeans size 32]"
            else:
                return "search[blue denim jeans]"
        elif "laptop" in instruction_lower and "1000" in instruction_lower:
            return "search[laptop under 1000 dollars]"
        elif "shirt" in instruction_lower and "red" in instruction_lower:
            return "search[red cotton shirt]"
        elif "mouse" in instruction_lower and "wireless" in instruction_lower:
            return "search[wireless mouse with good ratings]"
        else:
            # ä»æŒ‡ä»¤ä¸­æå–å…³é”®è¯
            keywords = self._extract_keywords(instruction)
            if keywords:
                return f"search[{keywords}]"
            else:
                return "search[product]"
    
    def _extract_keywords(self, instruction):
        """ä»æŒ‡ä»¤ä¸­æå–å…³é”®è¯"""
        # ç§»é™¤å¸¸è§åŠ¨è¯å’Œä»‹è¯
        stop_words = {'find', 'get', 'buy', 'purchase', 'search', 'for', 'a', 'an', 'the', 'with', 'in', 'under', 'over'}
        words = instruction.lower().split()
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        if keywords:
            return ' '.join(keywords[:3])  # å–å‰3ä¸ªå…³é”®è¯
        else:
            return "product"
    
    def _calculate_log_prob(self, outputs, input_length):
        """è®¡ç®—ç”Ÿæˆåºåˆ—çš„å¯¹æ•°æ¦‚ç‡"""
        try:
            # è·å–ç”Ÿæˆçš„token IDsï¼ˆæ’é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
            generated_sequences = outputs.sequences[:, input_length:]
            scores = outputs.scores
            
            if not scores:
                return 0.0
            
            log_probs = []
            for i, score in enumerate(scores):
                if i >= generated_sequences.size(1):
                    break
                # è®¡ç®—æ¯ä¸ªä½ç½®çš„å¯¹æ•°æ¦‚ç‡
                log_prob = torch.log_softmax(score, dim=-1)
                # è·å–å®é™…ç”Ÿæˆtokençš„å¯¹æ•°æ¦‚ç‡
                token_log_prob = log_prob[0, generated_sequences[0, i]]
                log_probs.append(token_log_prob)
            
            if log_probs:
                return torch.stack(log_probs).mean().item()
            else:
                return 0.0
                
        except Exception as e:
            print(f"å¯¹æ•°æ¦‚ç‡è®¡ç®—é”™è¯¯: {e}")
            return 0.0
    
    def get_text_embedding(self, text):
        """è·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤ºï¼ˆç”¨äºç¼“å­˜é”®ï¼‰"""
        if not text:
            return torch.zeros(512)
            
        try:
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.llm(**inputs, output_hidden_states=True)
                # ä½¿ç”¨æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å‡å€¼
                embedding = outputs.hidden_states[-1].mean(dim=1).squeeze()
                return embedding.cpu()
                
        except Exception as e:
            print(f"æ–‡æœ¬åµŒå…¥é”™è¯¯: {e}")
            return torch.zeros(512)
    
    def forward(self, input_ids, attention_mask):
        """å‰å‘ä¼ æ’­ç”¨äºè®­ç»ƒ"""
        outputs = self.llm(input_ids, attention_mask=attention_mask, output_hidden_states=True)
        return outputs.logits, outputs.hidden_states[-1]

# æµ‹è¯•å‡½æ•°
def test_qwen_agent():
    """æµ‹è¯•Qwenæ™ºèƒ½ä½“"""
    print("ğŸ§ª æµ‹è¯•Qwenæ™ºèƒ½ä½“...")
    
    agent = QwenRAGENAgent()
    
    test_cases = [
        ("æ¨¡æ‹Ÿç½‘é¡µ - æœç´¢é¡µé¢", "Purchase a classic blanket"),
        ("æ¨¡æ‹Ÿç½‘é¡µ - å•†å“åˆ—è¡¨", "Get a blue jeans in size 32"),
        ("æ¨¡æ‹Ÿç½‘é¡µ - é¦–é¡µ", "Find a laptop under $1000")
    ]
    
    for i, (obs, instruction) in enumerate(test_cases):
        print(f"\nğŸ“ æµ‹è¯•æ¡ˆä¾‹ {i+1}: {instruction}")
        think, action, log_prob, full = agent.generate_webshop_response(obs, instruction)
        print(f"ğŸ’­ æ€è€ƒ: {think}")
        print(f"ğŸ¯ åŠ¨ä½œ: {action}")
        print(f"ğŸ“Š å¯¹æ•°æ¦‚ç‡: {log_prob:.4f}")
        print("-" * 50)

if __name__ == "__main__":
    test_qwen_agent()